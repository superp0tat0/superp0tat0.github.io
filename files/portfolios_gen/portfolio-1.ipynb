{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img\n",
    "  src=\"https://www.wei-siyi.com/posts/mcts_port/header.png\"\n",
    "  alt=\"The beautiful MDN logo.\">\n",
    "  <figcaption>Asymmetric tree growth</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Monte Carlo Tree Search with board games\n",
    "\n",
    "Monte Carlo Tree Search is one famous technique which has been widely used in zero-sum games. MCTS was first introduced in 2006, it is a heuristic search algorithm for decision processes. It combines knowledge from Tree Data Structure, Game Theory and the Monte Carlo Sampling.\n",
    "\n",
    "MCTS is highly similar to MinMax Tree Search. MinMax Tree Search is also a search algorithm that \"Maximize the minimum gain\". In short, it was designed for zero-sum games to maximize the minimum reward at every decisions. However, MinMax Tree Search is a greedy algorithm, it will search every possible end node of the game states, then justify who is the winner and return the status to the parents' nodes. MinMax could achieve a very high winning rate. But it also has one critical flaw. MinMax Tree has a quadratic time complexity $O(b^m)$. This flaw limits its usage in lots of board games like GO. To deal with this critical flaw, MCTS then been introduced to reduce time complexity. One of the most famous example, AlphaGO, combined both MCTS and deep learning to improve Monte Carlo Sampling and achieved astounding results.\n",
    "\n",
    "The core idea of Monte Carlo Tree is simple: Use Monte Carlo to reduce time complexity. Traditionally, MinMax Tree will expand every possible end state and evaluate all of them. However, we could improve this process using Monte Carlo Methods. Since at each incomplete game state, we could expand all the possible moves, then justify the winning rate by running simulations at each move. Then the estimated winning rate will converge to the true winning rate as the amount of simulations increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Demo of MCTS with 4x4 Gomoku\n",
    "\n",
    "In the following example we could see MCTS could plays really well with human players.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle of Operation for MCTS\n",
    "\n",
    "We want to formulate the process of Monte Carlo Tree first. If we treat a Monte Carlo Tree search as a black box function. The input of this function will be the current game state. The output of this function will be the next move. Given an example of Tik-Tak-Toe. The input will be the game board and the output will be the estimated optimum move for the given game board. There are 4 main steps in Monte Carlo Tree Search:\n",
    "* **Selection Process:** The selection start from root node R. Then keep select the successive child nodes **strategically** until one leaf node L is reached. We treat the root node R as the input (current game state) of the function. The leaf node L could be any possible successive child node where no simulations has been initialized.\n",
    "* **Expansion Process:** After we select the valid child nodes L. Unless L already be the end state, which either represent WIN=1, LOSE=-1 or a DRAW=0. We will expand node L by creating all possible child nodes of L. Then we randomly select one child node C represent the game state after any possible move from L.\n",
    "* **Simulation Process:** Once we randomly select a child node C. We want to get the estimated winning rate from the current game state C. We then run the Monte Carlo simulations on current game state until the game ends.\n",
    "* **Back Propagation:** We update the information C to R using the results of the simulation.\n",
    "* **Optimization:** We then return the move of current game state with the highest estimated winning rate.\n",
    "\n",
    "![](https://www.wei-siyi.com/posts/mcts_port/MCTS_process.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.378304Z",
     "start_time": "2021-05-03T17:39:49.975318Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "class Imcts(object):\n",
    "    def select(self):\n",
    "        \"\"\"\n",
    "        Selection Process: Select the next node for expansion\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def expand(self):\n",
    "        \"\"\"\n",
    "        Expansion Process: Expand the current node\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def simulate(self):\n",
    "        '''\n",
    "        Simulation Process: Simulate the from current state for multiple times\n",
    "        until we get a reasonable estimate of the winning probability\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def backprop(self):\n",
    "        '''\n",
    "        Back Propagation: Update the simulate information from current node to root node\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Optimization: Run the whole process and return the optimized move\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to define a proper data structure to store the leaf node for the tree. Remember the childrens of each node are only limited by the games, which means there could be multiple children for each node.\n",
    "We also need to store the node's parent. The current board and player state for this node.\n",
    "MCTS also needs the times this node getting visited and the times this node wins. We will explain why we need them later. They are very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.384920Z",
     "start_time": "2021-05-03T17:39:50.380770Z"
    }
   },
   "outputs": [],
   "source": [
    "class node():\n",
    "    def __init__(self, parent, board, player, action = None, n = 0, w = 0, q = 0):\n",
    "        self.child = {} # The child of this node\n",
    "        self.parent = parent # The parent of this node\n",
    "        self.board = board # THe game board (nxn np array) of this board)\n",
    "        self.player = player # Current player of this board state (Who should play at this step)\n",
    "        self.action = action # The action that could get you from previous board to the current board\n",
    "        self.n = n # The times this node getting visited\n",
    "        self.w = w # The times this node wins (updated from the child node)\n",
    "        self.q = q # The winning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MCTS Constructor**: \n",
    "\n",
    "When we create the MCTS object. We need to set some important parameters which we will need them later. Those important parameters include the maximum simulation times for each decision. The maximum depth for each simulation, and **the explore constant**. We will leave the usage of those parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.391484Z",
     "start_time": "2021-05-03T17:39:50.387108Z"
    }
   },
   "outputs": [],
   "source": [
    "class MCTS(Imcts):\n",
    "    def __init__(self, iterations=100, max_depth=5, game_board=None, win_mark=3, player=None, explore_constant=np.sqrt(2)):\n",
    "        self.iterations = iterations #The amount of simulations for each decision\n",
    "        self.max_depth = max_depth #The maximum depth for each simulation\n",
    "        self.explore_constant = explore_constant #Explore constant for UCT\n",
    "\n",
    "        self.win_mark = win_mark #How many chess in a line wins\n",
    "        self.game_board = game_board #Current game board\n",
    "        self.player = player #Which player plays\n",
    "        self.total_n = 0 #Totol simulation times\n",
    "\n",
    "        self.size = len(game_board) #The size of the game board\n",
    "        \n",
    "        self.tree = node(None, self.game_board, self.player) #Tree structure to store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection**:\n",
    "\n",
    "Next, follow the previous documentation we created for selection. We want to select the next node for expansion. However, this is actually a hard question.\n",
    "\n",
    "Think about it, we have run a lot of simulations. We might have these scenarios. We have our first node which has 10000 simulations and the winning rate is about 80%. For the second node which has 100 simulations but only with 60% winning rate. If we want to select one of them to explore more, which one will be the best choice?\n",
    "\n",
    "The answer is hard to tell. You may want to say the first one is definitely better. However, for the first node it has been explored a lot, which means there will be little change of its expected winning rate. However, for the second node, even though the current winning rate is not as good as the first node, it is still worthy to explore consider the little amount of simulations.\n",
    "\n",
    "This tradeoff is called exploitation and exploration tradeoff. To solve this problem, Levente Kocsis and Csaba Szepesv√°ri introduced the UCB1 formula to solve this problem. Assume $w_i$ to be the winning times and $n_i$ to be the simulation times of making movement from node $i$, and $N_i$ be the total simulation times of node $i$. Then then UCB1 formula could be derived below:\n",
    "\n",
    "\n",
    "$$UCB = \\frac{w_i}{n_i} + c\\sqrt{\\frac{log(N_i)}{n_i}}$$\n",
    "\n",
    "$c$ represents the exploration parameter, which is usually being set to root of 2. This formula is quite intuitive. The first term represents the winning rate of taking the current movement, the second term represents the inverse of the simulation proportion of current movements. Overall we want both of the terms to be large, which gives us a movement with large winning probability and few simulations. Such movements are more worthy to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.399106Z",
     "start_time": "2021-05-03T17:39:50.393300Z"
    }
   },
   "outputs": [],
   "source": [
    "    def select(self):\n",
    "            \"\"\"\n",
    "            Selection Process: Select the next node for expansion\n",
    "            \"\"\"\n",
    "            node_founded = False\n",
    "            current_node = self.tree\n",
    "            depth = 0\n",
    "\n",
    "            while not node_founded:\n",
    "                child_nodes = current_node.child\n",
    "                depth += 1\n",
    "\n",
    "                if(len(child_nodes) == 0): #If this node is not yet explored\n",
    "                    node_founded = True\n",
    "                else:                     #Else, select one from the child (UCT) node and keep searching\n",
    "                    #Choose the node with maximum UCB value\n",
    "                    max_uct_value = -100.0\n",
    "                    for child_node in child_nodes:\n",
    "                        w = child_node.w\n",
    "                        n = child_node.n\n",
    "                        total_n = self.total_n\n",
    "                        if(n == 0): n = 1e-4 #Avoid numerical error\n",
    "\n",
    "                        #UCB formula\n",
    "                        exploitation_value = w / n\n",
    "                        exploration_value  = np.sqrt(np.log(total_n)/n)\n",
    "                        uct_value = exploitation_value + self.explore_constant * exploration_value\n",
    "\n",
    "                        if uct_value > max_uct_value:\n",
    "                            max_uct_value = uct_value\n",
    "                            current_node = child_node\n",
    "            return depth, current_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expansion**:\n",
    "\n",
    "For the expansion process, we will follow our documention. We expand node L by creating all possible child nodes of L. Then we randomly select one child node C represent the game state after some possible move from L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.409707Z",
     "start_time": "2021-05-03T17:39:50.401567Z"
    }
   },
   "outputs": [],
   "source": [
    "    def expand(self, leaf_node):\n",
    "        \"\"\"\n",
    "        Expansion Process: Expand the current node\n",
    "        \"\"\"\n",
    "        leaf_board = leaf_node.board\n",
    "        winner = get_winners(leaf_board, self.win_mark)\n",
    "        avaliable_actions, _ = get_valid_actions(leaf_board)\n",
    "\n",
    "        current_node = leaf_node\n",
    "        if (winner is None): # Not a terminal state\n",
    "            avaliable_childs = []\n",
    "            for action in avaliable_actions: #Grow all the avaliable actions\n",
    "                board = deepcopy(leaf_node.board)\n",
    "                current_player = leaf_node.player\n",
    "                \n",
    "                #Update the childnode's board status\n",
    "                if(current_player == 'o'):\n",
    "                    next_player = 'x'\n",
    "                    board[action] = 1\n",
    "                else:\n",
    "                    next_player = 'o'\n",
    "                    board[action] = -1\n",
    "\n",
    "                new_child = node(current_node, board, next_player, action=action)\n",
    "                avaliable_childs.append(new_child)\n",
    "            \n",
    "            #Randomly selection one child to be the node for simulation\n",
    "            current_node.child = avaliable_childs\n",
    "            random_index = np.random.randint(low=0, high=len(avaliable_childs), size=1)\n",
    "            current_node = avaliable_childs[random_index[0]]\n",
    "        \n",
    "        return current_node "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T04:47:09.188828Z",
     "start_time": "2021-05-03T04:47:09.185091Z"
    }
   },
   "source": [
    "**Simulation**:\n",
    "\n",
    "Once we randomly select a child node C. We want to get the estimated winning rate from the current game state C. We then run the Monte Carlo simulations on current game state until the game ends. Here we used the most simple rollout policy: simply assign the chess randomly until one player to be the winner. However, this policy is not very effective, and we could replaced with some more efficient policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.418424Z",
     "start_time": "2021-05-03T17:39:50.411562Z"
    }
   },
   "outputs": [],
   "source": [
    "    def simulate(self, child_node):\n",
    "        self.total_n += 1 #Total simulation coutner\n",
    "        board = deepcopy(child_node.board)\n",
    "        previous_player = child_node.player\n",
    "        winner = get_winners(board, self.win_mark)\n",
    "        simulate_time = 0\n",
    "        \n",
    "        #Randomly assign chess until someone win\n",
    "        while(winner is None):\n",
    "            #Different simulate policy\n",
    "            if(self.IS):\n",
    "                avaliable_actions, runtime = get_neighbor_actions(board, False) #Improved MCTS\n",
    "            else:\n",
    "                avaliable_actions, runtime = get_valid_actions(board, False) #Naive MCTS\n",
    "\n",
    "            #Roll out trick random assign the chesses\n",
    "            random_index = np.random.randint(low=0, high=len(avaliable_actions), size=1)[0]\n",
    "            action = avaliable_actions[random_index]\n",
    "            simulate_time += runtime\n",
    "\n",
    "            if previous_player == 'o':\n",
    "                current_player = 'x'\n",
    "                board[action] = -1\n",
    "            else:\n",
    "                current_player = 'o'\n",
    "                board[action] = 1\n",
    "            \n",
    "            previous_player = current_player\n",
    "            winner = get_winners(board, self.win_mark)\n",
    "        \n",
    "        return winner, simulate_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backprop**:\n",
    "\n",
    "Finally, when we want to update the from the current simulated node. We need to update the simulation times, winning times and the winning rate for each node on this path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.426396Z",
     "start_time": "2021-05-03T17:39:50.420686Z"
    }
   },
   "outputs": [],
   "source": [
    "    def backprop(self, child_node, winner):\n",
    "        player = self.tree.player\n",
    "\n",
    "        if(winner == \"draw\"):\n",
    "            reward = 0\n",
    "        elif(winner == player):\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        #Update gaming info from child node all the way to the root node\n",
    "        finished = False\n",
    "        while(not finished):\n",
    "            child_node.n += 1\n",
    "            child_node.w += reward\n",
    "            child_node.q = child_node.w / child_node.n \n",
    "            parent_node = child_node.parent\n",
    "            if(parent_node is not None):\n",
    "                child_node = parent_node\n",
    "            else:\n",
    "                finished = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**:\n",
    "\n",
    "Then we just need to simulate for given times following selection, expansion, simulation and backpropagation process. At the end we will return the optimum action with the maximum winning probability. In this function you could see some helper function that we will define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.438361Z",
     "start_time": "2021-05-03T17:39:50.430117Z"
    }
   },
   "outputs": [],
   "source": [
    "    def run(self, self_compete = False): #Self compete on/off, the return type is different.\n",
    "        start_time = time.time()\n",
    "        simulate_time = 0\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            depth, selected_node = self.select()\n",
    "            child_node = self.expand(selected_node)\n",
    "            winner, runtime = self.simulate(child_node)\n",
    "            simulate_time += runtime\n",
    "\n",
    "            self.backprop(child_node, winner)\n",
    "\n",
    "            if(depth > self.max_depth): #If depth over max depth, kill the loop and assess based on current information\n",
    "                break\n",
    "        \n",
    "        #Select the best action\n",
    "        current_node = self.tree\n",
    "        possible_nodes = current_node.child\n",
    "        best_q = -100\n",
    "        for node in possible_nodes:\n",
    "            if(node.q > best_q):\n",
    "                best_q = node.q\n",
    "                best_action = node.action\n",
    "        self.p_plot(possible_nodes) #Probability plots\n",
    "        run_time = time.time() - start_time - simulate_time\n",
    "\n",
    "        if(not self_compete): #Visulization require int action type\n",
    "            best_action = best_action[0] * self.size + best_action[1]\n",
    "\n",
    "        return best_action, best_q, depth, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "Now we have defined the tree nodes and the MCTS algorithm class for our game board. However, there are other helper functions we need to define. \n",
    "\n",
    "For example, to assess the winner of current game state. We need to assess whether there are n chess lies in a line. Which could be horizontal, vertical or diagonal. It would be easy to assess on a nxn board. However, what if we want to assess (n-1) win marks on a nxn board. To solve this question, we could use a shift window to help us determine the winner.\n",
    "\n",
    "Another helper function would be the get the valid actions for the game state. You surely do not want someone place a chess on a position you already placed. To avoid this, we need to get all the avaliable positions of the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.450617Z",
     "start_time": "2021-05-03T17:39:50.440902Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_winners(board, win_mark):\n",
    "    \"\"\"\n",
    "    get the winner of this board\n",
    "    arg:\n",
    "    - board state\n",
    "    return:\n",
    "    \"o\", \"x\", \"draw\", None (not ended)\n",
    "    \"\"\"\n",
    "\n",
    "    #return who wins\n",
    "    def __who_wins(sums, win_mark):\n",
    "        if np.any(sums == win_mark): return 'o'\n",
    "        if np.any(sums == -win_mark): return 'x'\n",
    "        return None\n",
    "\n",
    "    #jusify the winner\n",
    "    def __is_terminal_in_conv(leaf_state, win_mark):\n",
    "        # check row/col\n",
    "        for axis in range(2):\n",
    "            sums = np.sum(leaf_state, axis=axis)\n",
    "            result = __who_wins(sums, win_mark)\n",
    "            if result is not None: return result\n",
    "        # check diagonal\n",
    "        for order in [-1,1]:\n",
    "            diags_sum = np.sum(np.diag(leaf_state[::order]))\n",
    "            result = __who_wins(diags_sum, win_mark)\n",
    "            if result is not None: return result\n",
    "        return None\n",
    "\n",
    "    n_rows_board = len(board)\n",
    "    #Use a shift window to check the winner\n",
    "    window_positions = range(n_rows_board - win_mark + 1)\n",
    "\n",
    "    #Jusify the winner looping board by win_mark \n",
    "    for row in window_positions:\n",
    "        for col in window_positions:\n",
    "            window = board[row:row+win_mark, col:col+win_mark]\n",
    "            winner = __is_terminal_in_conv(window, win_mark)\n",
    "            if winner is not None:\n",
    "                return winner\n",
    "\n",
    "    if not np.any(board == 0):\n",
    "        return 'draw'\n",
    "    return None\n",
    "\n",
    "def get_valid_actions(board, timer = False):\n",
    "    '''\n",
    "    return all possible action in current leaf state\n",
    "    in:\n",
    "    - board\n",
    "    out:\n",
    "    - set of possible actions (row,col) - start from 0\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    actions = []\n",
    "    state_size = len(board)\n",
    "\n",
    "    for i in range(state_size):\n",
    "        for j in range(state_size):\n",
    "            if board[i][j] == 0:\n",
    "                actions.append((i, j))\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    if(timer): return actions, runtime\n",
    "    else: return actions, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to get all the valid actions may not be an effective selection. Some positions have much less value to explore. For example, in tic-tak-toe or gomoku, we only have two strategy: either block opponents movement or grow your own line of chess. To do both of the activities, you had to have place a chess where its neighbor is not empty. In another word, we could select the avaliable actions where only its neighbor is not empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.458057Z",
     "start_time": "2021-05-03T17:39:50.453022Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_neighbor_actions(board, timer = False):\n",
    "    '''\n",
    "    return all neighbot actions in current leaf state\n",
    "    in:\n",
    "    - board\n",
    "    out:\n",
    "    - set of possible actions (row,col) - start from 0\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    actions = set()\n",
    "    valid_actions, _ = get_valid_actions(board)\n",
    "    state_size = len(board)\n",
    "\n",
    "    for i in range(state_size):\n",
    "        for j in range(state_size):\n",
    "            if board[i][j] != 0:\n",
    "                for index_a in range(i-1, i+2):\n",
    "                    for index_b in range(j-1,j+2):\n",
    "                        if((index_a, index_b) in valid_actions):\n",
    "                            actions.add((index_a, index_b))\n",
    "    run_time = time.time() - start_time\n",
    "\n",
    "    if(timer): return list(actions), run_time\n",
    "    else: return list(actions), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code of MCTS\n",
    "\n",
    "Now, with the full code of MCTS, we could play some tic-tak-toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:39:50.486391Z",
     "start_time": "2021-05-03T17:39:50.460195Z"
    }
   },
   "outputs": [],
   "source": [
    "class MCTS(Imcts):\n",
    "    def __init__(self, iterations=100, max_depth=5, game_board=None, win_mark=3, player=None, explore_constant=np.sqrt(2), IS = True):\n",
    "        self.iterations = iterations\n",
    "        self.max_depth = max_depth\n",
    "        self.explore_constant = explore_constant\n",
    "\n",
    "        self.win_mark = win_mark\n",
    "        self.game_board = game_board\n",
    "        self.player = player\n",
    "        self.total_n = 0\n",
    "\n",
    "        self.size = len(game_board)\n",
    "        \n",
    "        self.tree = node(None, self.game_board, self.player)\n",
    "        self.IS = IS #Importance sampling on / off\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\"\n",
    "        Select the next node for expansion\n",
    "        \"\"\"\n",
    "        node_founded = False\n",
    "        current_node = self.tree\n",
    "        depth = 0\n",
    "        \n",
    "        while not node_founded:\n",
    "            child_nodes = current_node.child\n",
    "            depth += 1\n",
    "\n",
    "            if(len(child_nodes) == 0): #If this node is not yet explored\n",
    "                node_founded = True\n",
    "            else:                     #Else, select one from the child (UCT) node and keep searching\n",
    "                max_uct_value = -100.0\n",
    "                for child_node in child_nodes:\n",
    "                    w = child_node.w\n",
    "                    n = child_node.n\n",
    "                    total_n = self.total_n\n",
    "                    if(n == 0): n = 1e-4 #Avoid numerical error\n",
    "\n",
    "                    #UCT formula\n",
    "                    exploitation_value = w / n\n",
    "                    exploration_value  = np.sqrt(np.log(total_n)/n)\n",
    "                    uct_value = exploitation_value + self.explore_constant * exploration_value\n",
    "\n",
    "                    if uct_value > max_uct_value:\n",
    "                        max_uct_value = uct_value\n",
    "                        current_node = child_node\n",
    "        return depth, current_node\n",
    "\n",
    "    def expand(self, leaf_node):\n",
    "        \"\"\"\n",
    "        Expand the current node\n",
    "        \"\"\"\n",
    "        leaf_board = leaf_node.board\n",
    "        winner = get_winners(leaf_board, self.win_mark)\n",
    "        avaliable_actions, _ = get_valid_actions(leaf_board)\n",
    "\n",
    "        current_node = leaf_node\n",
    "        if (winner is None): # Not a terminal state\n",
    "            avaliable_childs = []\n",
    "            for action in avaliable_actions: #Grow all the avaliable actions\n",
    "                board = deepcopy(leaf_node.board)\n",
    "                current_player = leaf_node.player\n",
    "\n",
    "                if(current_player == 'o'):\n",
    "                    next_player = 'x'\n",
    "                    board[action] = 1\n",
    "                else:\n",
    "                    next_player = 'o'\n",
    "                    board[action] = -1\n",
    "\n",
    "                new_child = node(current_node, board, next_player, action=action)\n",
    "                avaliable_childs.append(new_child)\n",
    "            current_node.child = avaliable_childs\n",
    "            random_index = np.random.randint(low=0, high=len(avaliable_childs), size=1)\n",
    "            current_node = avaliable_childs[random_index[0]]\n",
    "        \n",
    "        return current_node \n",
    "\n",
    "\n",
    "    def simulate(self, child_node):\n",
    "        self.total_n += 1\n",
    "        board = deepcopy(child_node.board)\n",
    "        previous_player = child_node.player\n",
    "        winner = get_winners(board, self.win_mark)\n",
    "        simulate_time = 0\n",
    "\n",
    "        while(winner is None):\n",
    "            if(self.IS): avaliable_actions, runtime = get_neighbor_actions(board, False) #Improved MCTS\n",
    "            else: avaliable_actions, runtime = get_valid_actions(board, False) #Naive MCTS\n",
    "\n",
    "            #Roll out trick random assign the chesses\n",
    "            random_index = np.random.randint(low=0, high=len(avaliable_actions), size=1)[0]\n",
    "            action = avaliable_actions[random_index]\n",
    "            simulate_time += runtime\n",
    "\n",
    "            if previous_player == 'o':\n",
    "                current_player = 'x'\n",
    "                board[action] = -1\n",
    "            else:\n",
    "                current_player = 'o'\n",
    "                board[action] = 1\n",
    "            \n",
    "            previous_player = current_player\n",
    "            winner = get_winners(board, self.win_mark)\n",
    "        \n",
    "        return winner, simulate_time\n",
    "\n",
    "    def backprop(self, child_node, winner):\n",
    "        player = self.tree.player\n",
    "\n",
    "        if(winner == \"draw\"):\n",
    "            reward = 0\n",
    "        elif(winner == player):\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        #Update gaming info from child node all the way to the root node\n",
    "        finished = False\n",
    "        while(not finished):\n",
    "            child_node.n += 1\n",
    "            child_node.w += reward\n",
    "            child_node.q = child_node.w / child_node.n \n",
    "            parent_node = child_node.parent\n",
    "            if(parent_node is not None):\n",
    "                child_node = parent_node\n",
    "            else:\n",
    "                finished = True\n",
    "\n",
    "    def run(self, self_compete = False): #Self compete on/off, the return type is different.\n",
    "        start_time = time.time()\n",
    "        simulate_time = 0\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            depth, selected_node = self.select()\n",
    "            child_node = self.expand(selected_node)\n",
    "            winner, runtime = self.simulate(child_node)\n",
    "            simulate_time += runtime\n",
    "            self.backprop(child_node, winner)\n",
    "\n",
    "            if(depth > self.max_depth): #If depth over max depth, kill the loop and assess based on current information\n",
    "                break\n",
    "        \n",
    "        #Select the best action\n",
    "        current_node = self.tree\n",
    "        possible_nodes = current_node.child\n",
    "        best_q = -100\n",
    "        for node in possible_nodes:\n",
    "            if(node.q > best_q):\n",
    "                best_q = node.q\n",
    "                best_action = node.action\n",
    "        run_time = time.time() - start_time - simulate_time\n",
    "\n",
    "        if(not self_compete): #Visulization require int action type\n",
    "            best_action = best_action[0] * self.size + best_action[1]\n",
    "\n",
    "        return best_action, best_q, depth, run_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple visualization. We could let the MCTS with the rollout policy play with the MCTS with the improved position selection process. You could see the MCTS Improved often has the higher winning probability. But you are free to check it your self. For the better visualization and interactive play showed in the video. You could view my code at github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:40:14.180740Z",
     "start_time": "2021-05-03T17:39:50.488317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1  0  0  0  0]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[ 0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1  0  0  0  0]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[ 0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1  0  1  0  0]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[ 0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1  0  1 -1  0]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[ 0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 1  0  0  0  0]\n",
      " [ 1  0  1 -1  0]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[ 0 -1  0  0  0]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 1  0  0  0  0]\n",
      " [ 1  0  1 -1  0]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[ 0 -1  0  0  0]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 1  0  0  1  0]\n",
      " [ 1  0  1 -1  0]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[-1 -1  0  0  0]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 1  0  0  1  0]\n",
      " [ 1  0  1 -1  0]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[-1 -1  0  0  0]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 1  0  0  1  0]\n",
      " [ 1  0  1 -1  0]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[-1 -1  0  0  0]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 1  0  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[-1 -1  0  0  0]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[-1 -1  0  0 -1]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[-1 -1  0  0 -1]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 1  0  1  1  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[-1 -1 -1  0 -1]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 1  0  1  1  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[-1 -1 -1  1 -1]\n",
      " [ 0  0 -1 -1  0]\n",
      " [ 1  0  1  1  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[-1 -1 -1  1 -1]\n",
      " [-1  0 -1 -1  0]\n",
      " [ 1  0  1  1  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[-1 -1 -1  1 -1]\n",
      " [-1  1 -1 -1  0]\n",
      " [ 1  0  1  1  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Rollout Turn\n",
      "[[-1 -1 -1  1 -1]\n",
      " [-1  1 -1 -1 -1]\n",
      " [ 1  0  1  1  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "----------------------------\n",
      "MCTS Improved Position Turn\n",
      "[[-1 -1 -1  1 -1]\n",
      " [-1  1 -1 -1 -1]\n",
      " [ 1  1  1  1  0]\n",
      " [ 1  1  0  1  0]\n",
      " [ 1  0  1 -1 -1]]\n",
      "The winner is mcts_improved\n"
     ]
    }
   ],
   "source": [
    "current_player = 1\n",
    "mcts_player = -1\n",
    "win_mark = 4\n",
    "board_shape = [5, 5]\n",
    "winner_map = {\"x\": \"mcts_rollout\", \"o\": \"mcts_improved\"}\n",
    "winner = None\n",
    "\n",
    "game_board = np.zeros(board_shape, dtype=int)\n",
    "\n",
    "\n",
    "#Self Competence\n",
    "while winner is None:\n",
    "    print(\"----------------------------\")\n",
    "    if current_player == mcts_player: #IS MC player\n",
    "        print(\"MCTS Rollout Turn\")\n",
    "        mcts = MCTS(iterations=400, max_depth=30, win_mark=win_mark, game_board=game_board, player=current_player)\n",
    "        best_action, best_q, depth, run_time = mcts.run(self_compete=True)\n",
    "        game_board[best_action] = -1\n",
    "    else: #Naive MC player\n",
    "        mcts = MCTS(iterations=400, max_depth=30, win_mark=win_mark, game_board=game_board, player=current_player, IS=False)\n",
    "        print(\"MCTS Improved Position Turn\")\n",
    "        best_action, best_q, depth, run_time = mcts.run(self_compete=True)\n",
    "        game_board[best_action] = 1\n",
    "    \n",
    "    print(game_board)\n",
    "    winner = get_winners(game_board, win_mark)\n",
    "    current_player = -current_player\n",
    "\n",
    "print(\"The winner is \"+winner_map[winner])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

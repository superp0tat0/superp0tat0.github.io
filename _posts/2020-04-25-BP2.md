---
title: 'What is behind Batch Normalization'
date: 2020-04-25
permalink: /posts/2020/04/BP2/
tags:
  - statistics
  - machine learning
  - batch normalization
  - layer normalization
  - internal covariate shift
  - data mining
  - regulizer
  - data science
---
The discussion of ideology of Batch Normalization and how should we use it?

Introduction
======
This is a reading project I did with my teammates for Analysis of Big Data I took at the final year. In this project we further analyze the ideology of Batch Normalization. Batch Normalization is widly accepted as a popular method for boosting the trainning process. During my research on Kaggle competition. I found even though lots of people like to use it during trainning process. However, they often use it either inefficient or incorrectly. To use it correctly, we need to understand the ideology behind it. There is still an ongoing debate on the ideology behind Batch Normaliation. So the main topic of this blog is to discuss this ongoing debate and let the readers understand how batch normalization works. Then we will talk about what is the proper way to use Batch Normaliation, and the variant of Batch Normalization. I have attached the ["project article link"](https://www.wei-siyi.com/files/D80_Final_Report.pdf) here. 

What is Batch Normalization?
======
The intuition behind Batch Normalization is simple. There is a common trick in Data Science and Statistics problems called "Normalization". Which has a easy to understand formula, Define $X$ to be the sample data, $\mu$ to be the sample mean and $\sigma$ to be the standard error.
$$\frac{X - \mu}{\sigma}$$
Use the sample to minus the mean of the sample mean, and divide by the sample standard error. This trick intend to add the consistency of the sample sets, and gave the stability to the model predictions.
Now back to Batch Normalization, it is commonly used in trainning Deep Neural Networks for a decrease in trainning time and increase for stability in trainning process. In deep neural networks, each layer could be treated as an independent optimization problem. Where each layer are optimize its learnable parameters by applying gradient descent and chain rule (AKA back propagation). If each layer could be treated as an optimization problem, then applying normalization only at the very first sample set would be not that efficient. Why don't we apply normalization before each layer? The answer is we could, but we need some modification.

Here is the algorithm for Batch Normalization. We denote $B$ to be a sample with size $m$, where the empirical mean and standard error for $B$ is $\mu_B$ and $\sigma_B$. For one specific layer of the network with $d$-dimention input, $x = (x^{(1)},...,x^{(d)})$, each dimension of its input is then normalized. Given $k$-th neuron in this given layer, the step-1 BN formula is shown as:
$\widehat{x_i^{(k)}} = \frac{x_i^{(k)} - \mu_B^{(k)}}{\sigma_B^{(k)} + \epsilon}$
Where $k \in [1,d]$, $i \in [1,m]$ and $\epsilon$ is added for avoiding numerical error when $\sigma$ is close to 0.
So far we are talking about the traditional normalization. But there is an important concept we missed. In neural network each neuron should learn a hidden feature. But not all the hidden feature will have mean 0 and variance 1. In other work, normalization might not be needed in some neurons. To correct our possible "errors", we need to add the function to correct it. To do this, we then apply step 2 of BN formula:
$y_i^{(k)} = \gamma^{(k)} \widehat{x_i^{(k)} + \beta^{(k)}}$
Where $\gamma^{(k)}$ and $\beta^{(k)}$ are learnable parameters. It was proven in the original papers so we wont mention further details. As we could see, for specific value of the given parameters we could let $y_i^{(k) = x_i^{(k)}}$. Which result we could at least restore our original value.

The whole algorithm can be represent in the following pseudo code from the original paper[1]:

Debate on ideology of Batch Normalization
======
The ideology of Batch Normalization was vague in the original paper. Sergey Ioffe and Christian Szegedy gave an intuitive explanation on how Batch Normalization could boost training by solving an problem called "Internal Covariate Shift".
It took me sometime to understand **"Internal Covariate Shift" (ICS)**. Basically it is decribing the phenomenon that the input distribtion of one specific neuron could change during the trainning process. Surely it could be good since it means the input distribution could get closer to the real hidden feature distribution. But it could also be bad since it may cause the instability of the trainning process without catching the hidden feature. In the original paper, authors believe Batch Normalization has the ability to avoid Internal Covaraite Shift by applying normalization and shift by learnable parameters.

Since this is an intuitive explanation. There is no proof for it yet. In a recently ["published paper"](https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf), some reseachers from MIT gave another explanation. They showed there is weak negative connection between Internal Covariate shift and the Improvement in Neural Network (PS: Which is the one I am confusing since based on my previous understand ICS does not have to be bad). They then showed Batch Normalization sometimes could even cause more ICS.

After that, they raised their conclusions on how Batch Normalization could help on improving the trainning process. Which I somehow agree, which is the **smoothness**. Not only Batch Normalization, I believe smoothness plays a key role in all optimization problem. Taking L2 regulizer or Normalization, a smooth model/loss function means great generalize ability. It could prevent your model from being too sensitive. Back to our problem, the authors managed to prove BN could set an upper bound to the un-smoothness in loss function. This part involve some mathmetics so I would suggest to read the original paper. In other word, we could prevent the loss function in layer optimization problem being too unsmooth.

However, the debate is still going. Some researchers believe BN actually helps on both side and the conclusion on there is weak connect between BN and ICS was founded on defective experiment. Since this is still a cutting edge debate. I will not say anything is definte.

What you need to be aware of Batch Normalization.
======
**Never apply DropOut with Batch Normalization.**Batch Normalization should be carefully implement, it is not a panacea. One common mistake is data scientists like to use it with DropOut. For those who do not know it means drop some neurons during trainning. However, this is wrong! First dropout is another regularize method, which is somehow similar to Batch Normalization. Second, by applying dropout the internal distribution will vary then its original distribution. This could cause Batch Normalization to generalize a distribution who have even weaker relation to the original distribution.

**Batch Normalization should not be applied for RNN(Recursive Neural Network).**


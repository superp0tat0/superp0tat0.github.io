---
title: 'Understand Batch Normalization'
date: 2020-04-25
permalink: /posts/2020/04/BP2/
tags:
  - machine learning
  - batch normalization
  - layer normalization
  - data science
---
The discussion of ideology of Batch Normalization and how should we use it?

Introduction
======
This blog comes from a reading project for Analysis of Big Data. In this project we further analyze the ideology of Batch Normalization. Batch Normalization is widely accepted as a popular method for boosting the training process. During some research on Kaggle competitions. I found even though lots of people like to use it for training neural networks. However, they often use it either inefficiently or incorrectly. To use it correctly, we better understand the ideology behind it. There is still an ongoing debate on the ideology behind Batch Normalization. So, the main topic of this blog is to discuss this ongoing debate and let the readers have a superficial understanding of Batch Normalization. Then we will talk about what is the proper way to use Batch Normalization, and the variants of Batch Normalization. I have attached the ["project article link"](https://www.wei-siyi.com/files/D80_Final_Report.pdf) here. 

What is Batch Normalization?
======
The intuition behind Batch Normalization is simple. There is a common trick in Data Science and Statistical problems called "Normalization". Which has a easy to understand formula, Define $X$ to be the sample data, $\mu$ to be the sample mean and $\sigma$ to be the standard error.
$$\frac{X - \mu}{\sigma}$$
Use the sample data minus the sample mean and divide by the standard error. This trick intends to add the consistency of the sample sets and stabilize the model predictions.
Now back to Batch Normalization, it is commonly used in training Deep Neural Networks for a decrease in training time and increase for stability. In deep neural networks, each layer could be treated as an independent optimization problem. Where each layer is optimizing its learnable parameters by applying gradient descent and chain rule (AKA back propagation). If each layer could be treated as an optimization problem, then applying normalization only before the first layer would be inefficient. Why don't we apply normalization before each layer? The answer is we could, but we need some modifications.


Here is the algorithm for Batch Normalization. We denote $B$ to be a sample set with size $m$, where the empirical mean and standard error for $B$ is $\mu_B$ and $\sigma_B$. For one specific layer of the network with $d$-dimension input, $x = (x^{(1)},...,x^{(d)})$, each dimension of its input is then normalized. Given $k$-th neuron in this given layer, the step-1 BN formula is shown as:
$\widehat{x_i^{(k)}} = \frac{x_i^{(k)} - \mu_B^{(k)}}{\sigma_B^{(k)} + \epsilon}$
Where $k \in [1,d]$, $i \in [1,m]$ and $\epsilon$ is added for avoiding numerical error when $\sigma$ is close to 0.
So far, we are talking about the traditional normalization. But there is an important concept we missed. Neural networks should learn hidden features. But not all the hidden feature will have mean 0 and variance 1. In other word, normalization might not be needed in some layers. To correct our possible "errors", we need to add a function that able to handle it. To do this, we then apply step 2 of BN formula:
$y_i^{(k)} = \gamma^{(k)}\widehat{x_i^{(k)}} + \beta^{(k)}$
Where $\gamma^{(k)}$ and $\beta^{(k)}$ are learnable parameters. It was proven in the original papers, so we would not mention further details. As we could see, for specific value of the given parameters we could let $y_i^{(k)} = x_i^{(k)}$. Which result we could at least restore our original value.

The whole algorithm can be found in the following pseudo code from the original paper ["[1]"](https://arxiv.org/pdf/1502.03167v3.pdf):

![original code](https://github.com/superp0tat0/superp0tat0.github.io/raw/master/posts/post3/algorithm.png)

Debate on ideology of Batch Normalization
======
The ideology of Batch Normalization was vague in the original paper. Sergey Ioffe and Christian Szegedy gave an intuitive explanation on how Batch Normalization could boost training by solving a problem called "Internal Covariate Shift".
It took me sometime to understand **"Internal Covariate Shift" (ICS)**. Basically, it is describing the phenomenon that the input distribution of one specific neuron could change during the training process. Surely it could be good since it means the input distribution could get closer to the real hidden feature's distribution. But it could also be bad since it may cause the instability of the training process when it is not fitting the hidden feature. In the original paper, authors believe Batch Normalization has the ability to avoid Internal Covariate Shift by applying normalization and shift by learnable parameters.

Since this is an intuitive explanation. There is no proof for it yet. In a recently published paper ["[2]"](https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf), some researchers from MIT gave another explanation. They showed there is weak or even no connections between Internal Covariate shift and the Improvement in Neural Network (PS: Which is the one I am confusing since based on my previous understand ICS does not have to be bad). They then showed Batch Normalization sometimes could even cause more ICS.

After that, they raised their conclusions on how Batch Normalization could help on improving the training process. Which I somehow agree, which is the **smoothness**. Not only Batch Normalization, I believe smoothness plays a key role in all optimization problem. Taking L2 regularization or Normalization, a smooth loss function means great generalize ability. It could prevent your model from being too sensitive. Now back to our problem, the authors managed to prove BN could set an upper bound to the un-smoothness in loss function. This part involves some mathematics so I would suggest reading the original paper. In other word, we could prevent the loss function in layer optimization problem being too unsmooth.

However, the debate is still ongoing. Some researchers believe BN actually helps on both side and the conclusion here is weak. Since the founding of BN and ICS was based on defective experiment. This is still a cutting-edge debate. I will not say anything is definite.

What you need to be aware of Batch Normalization.
======
People love use DropOut and Batch Normalization. It will act like install two antivirus softwires in your operating system. Consider the implicit regularization may be three.\\

**Never apply DropOut with Batch Normalization.**
Batch Normalization should be carefully implemented, it is not a panacea. One common mistake is to use it with DropOut. However, this is wrong! First dropout is another regularize method, which is somehow similar to Batch Normalization. Second, by applying dropout the internal distribution will vary then its original distribution. This could cause Batch Normalization to generalize a distribution who have even weaker relation to the original distribution.

**Batch Normalization should not be applied for RNN (Recursive Neural Network).**
We often use RNN to process the sequence data. The un-normalized sample distribution will carry the sequence information. Given an example, suppose we use RNN to predict a time sequence data. After we apply batch normalization, the time sequence information will get lost. In which case it is not a good idea to use BN. Instead, we could try another technic called Layer Normalization ["[3]"](https://arxiv.org/abs/1607.06450). The only difference if now we apply normalization at the output of neurons. This plot could show the difference between batch normalization which normalize across samples and layer normalization which normalize across features.

![Difference between BN and LN](https://github.com/superp0tat0/superp0tat0.github.io/raw/master/posts/post3/export.png)

Reference:

[1] ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167v3.pdf)

[2] ["How Does Batch Normalization Help Optimization?"](https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf)

[3] ["Layer Normalization"](https://arxiv.org/abs/1607.06450)

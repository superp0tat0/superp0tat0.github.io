---
title: 'How Riege/Lassp Regression works?'
date: 2020-02-13
permalink: /posts/2020/02/BP/
tags:
  - Machine Learning
  - Probability
  - Regulizer
---

Q: Why do we need Riege and Lasso Regression, and how they work intuitively? 

Introduction to Overfit
------
To fit a model, we need to introduce parameters to our model intend to fit the observations as accurate as possible. However, if we introduce more parameters than the distribution where our observations follow. We may encounter overfitting problem. To define overfitting, you could think our model is too "powerful", it could explain even the noise in our observations. The reason why overfit is bad for our prediction is simple. We want our model to be able to generalize the prediction, so it could predict future values based on experience. While overfit will hurt this generalization ability. Given an extreme case, my model could give the 100% accurate value based on the input, but only for the data it haven seen. It will give a random guess for the data it does not seen. Then my model is no better than a simple disk; Where the disk is also memorizing data.

Simple Overfit Example
------
How about we give a simple coding example to explain why overfit will hurt your model:
First import all the libraries that are required:
```
import numpy as np
import random as random
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
```
Then create our model class for fitting and visualization, here we only use Riege regression (polynomial regression with l2 regulizer):
```
class Demo_model:
    def __init__(self, w, mu, sig):
        self._x = np.expand_dims(np.arange(-10,10), axis=1)
        self._poly = np.poly1d(w)
        self._y = self.generate_data_1d(self._x, mu,sig)
        self._polywf = None
        self._ploywr2 = None
        self._ploywr2_value = None
    
    def plot(self):
        plt.plot(self._x, self._poly(self._x),'g')
        plt.plot(self._x, self._polywf(self._x),'b')
        #plt.plot(self._x, self._ploywr2_value,'r')
        plt.show()
            
    def generate_data_1d(self, x, mu, sig):
        num_samples = x.shape[0]
        # Generate noise
        noise = np.random.normal(loc=mu, scale=sig, size=x.shape)
        # Compute outputs y, equal to x^T w plus the noise
        y = self._poly(self._x) + noise
        return y
    
    def train(self, poly, r2):
        #Fit without regulizer
        self._polywf = np.poly1d(np.polyfit(self._x.T[0], self._y.T[0], poly))
        
        #Fit with l2 regulizer with lamda
        po = PolynomialFeatures(poly, include_bias=True)
        x = po.fit_transform(self._x.reshape(1, -1).T)
        rg = Ridge(alpha = r2)
        self._ploywr2 = rg.fit(x,self._y)
        self._ploywr2_value = self._ploywr2.predict(x)
```

Then we could set our target function to be a relatively simple function.
$$y = 0.1x^3 + 0.2x^2 + 0.1x + 4 + \epsilon$ where $\epsilon \sim \mathcal{N}(0,\sigma^2)$

```
random.seed(101)
w = np.array([0.1,0.2,0.1,4])
model = Demo_model(w,0,10)
```
Next, train our model with degree up to 100, and set our $\lambda = 1$
```
model.train(100,1)
model.plot()
```

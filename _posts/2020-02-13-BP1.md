---
title: 'How Riege/Lasso Regression works?'
date: 2020-02-13
permalink: /posts/2020/02/BP/
tags:
  - Machine Learning
  - Probability
  - Regulizer
---

Q: Why do we need Riege and Lasso Regression, and how they work intuitively? 

Introduction to Overfit
------
To fit a model, we need to introduce parameters to our model intend to fit the observations as accurate as possible. However, if we introduce more parameters than the distribution where our observations follow. We may encounter overfitting problem. 
To define overfitting, you could think our model is too "powerful", it could explain even the noise in our observations. The reason why overfit is bad for our prediction is simple. We want our model to be able to generalize the prediction, so it could predict future values based on experience. While overfit will hurt this generalization ability. Given an extreme case, my model could give the 100% accurate value based on the input, but only for the data it haven seen. It will give a random guess for the data it does not seen. Then my model is no better than a simple disk; Where the disk is also memorizing data.

Simple Overfit Example
------
How about we give a simple coding example to explain why overfit will hurt your model:
First import all the libraries that are required:
```
import numpy as np
import random as random
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
```
Then create our model class for fitting and visualization, here we only use Riege regression (polynomial regression with l2 regulizer):
```
class Demo_model:
    def __init__(self, w, mu, sig):
        self._x = np.expand_dims(np.arange(-10,10), axis=1)
        self._poly = np.poly1d(w)
        self._y = self.generate_data_1d(self._x, mu,sig)
        self._polywf = None
        self._ploywr2 = None
        self._ploywr2_value = None
    
    def plot(self):
        plt.plot(self._x, self._poly(self._x),'g') #The plot of original target function
        plt.plot(self._x, self._polywf(self._x),'b') #The plot of regression without any regulizer
        #plt.plot(self._x, self._ploywr2_value,'r') #The plot of riege regression
        plt.show()
            
    def generate_data_1d(self, x, mu, sig):
        num_samples = x.shape[0]
        # Generate noise
        noise = np.random.normal(loc=mu, scale=sig, size=x.shape)
        # Compute outputs y, equal to x^T w plus the noise
        y = self._poly(self._x) + noise
        return y
    
    def train(self, poly, r2):
        #Fit without regulizer
        self._polywf = np.poly1d(np.polyfit(self._x.T[0], self._y.T[0], poly))
        
        #Fit with l2 regulizer with lamda
        po = PolynomialFeatures(poly, include_bias=True)
        x = po.fit_transform(self._x.reshape(1, -1).T)
        rg = Ridge(alpha = r2)
        self._ploywr2 = rg.fit(x,self._y)
        self._ploywr2_value = self._ploywr2.predict(x)
```

Then we could set our target function to be a relatively simple function. In our following example we set our $sigma^2 = 10$, which is relatively high.
$y = 0.1x^3 + 0.2x^2 + 0.1x + 4 + \epsilon$ where $\epsilon \sim \mathcal{N}(0,\sigma^2)$

```
random.seed(101)
w = np.array([0.1,0.2,0.1,4])
model = Demo_model(w,0,10)
```
Next, train our model with degree up to 100, and set our $\lambda = 1$, we will talk about what $lambda$ means later, right now it means nothing since we have not plot it.
```
model.train(100,1)
model.plot()
```

![Overfit](https://github.com/superp0tat0/superp0tat0.github.io/raw/master/posts/post2/Riege_Original.png)

We could clearly see there is a significant error between the real function and our prediction function. Our model is too strong so it could fit the noise inside our model. To solve this problem, we could limit our hypothesis sets. Here you could think the hypothesis sets contains all the possibilities of our weights matrix. A larger hypothesis set will generally better than a smaller hypothesis set for reducing trainning error. But it will hurt model generalization ability.
So to set our hypothesis set to be smaller, the train error will goes up but it will improve the model generalization ability.

```
random.seed(101)
w = np.array([0.1,0.2,0.1,4])
model = Demo_model(w,0,10)
model.train(10,1)
model.plot()
```
![Less Overfit](https://github.com/superp0tat0/superp0tat0.github.io/raw/master/posts/post2/RO2.png)

Now it is better, but still a little bit overfit. But now there is a practical problem. How do you know which degree will perform the best. Of course you could loop all of them. But for some more complex functions the degree is not the only variable that you could manipulate. That is where the Regulizer comes out and make your life easier.
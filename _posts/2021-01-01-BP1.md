---
title: 'Gradient descent, Newton Method and simulated Newton Method.'
date: 2021-01-01
permalink: /posts/2021/01/BP1/
tags:
  - Machine Learning
  - Optimizer
---

Q: What are the differences between Gradient descent, Newton Method and simulated Newton Methods?

Anyone who is familiar with machine learning must be really familiar with gradient descent. It is a very useful method to find the optimum point for functions. Furthermore, if the given function is convex. Gradient descent is guaranteed to find the global optimum point of such function. Such property made gradient descent very popular in optimization tasks.

However, there are also some problems with gradient descent. Since the gradient descent only uses first order gradients. The converge speed is usually slow especially near the optimum point. This is also the reason why so many other variants of gradient descent have raised such as momentum, adam and other optimizers.

However in statistics, a more traditional optimizer is acceptable. Which uses second order gradient to find the optimum point. It is called "Newton Method". I will introduce the ideology and intuition behind both gradient descent and Newton Method. Then we will talk about why Newton Method is not popular in Machine Learning and their advantages and disadvantages.

To start from both gradient descent and Newton Method. We introduce function $f$. For $f$ we could find its first order tayler expansion and second order tayler expansion. To restore our memory. For any real or complex-valued function $f(x)$ that is infiniely differentiable at a real or complex number $a$. The taylor expansion of $f(x)$ at $a$ is:

$$f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \dots$$

Which implies for first order taylor expansion:

$$f(x) \approx f(x) + \frac{f'(a)}{1!}(x-a)$$

and the second order taylor expansion:

$$f(x) \approx f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2$$

### Gradient Descent
Now imagine we have a multi-dimensional optimization problem. When we are at step $k$. we would set $a = X_k$ because we want the taylor series of $f$ at $X_k$. Keep in mind $f'(X)$ now is the gradient of $f$ w.r.t to $X$.

$$f(X) \approx f(X_k) + \frac{f'(X_k)}{1!}(X-X_k)$$

Such approximated function is a monotone increasing function since all variables except $x$ are constants. Then we could update $X$ with given learning parameter $\lambda \in \mathbb{R^+}$

$$X_{k+1} \leftarrow X_{k} - \lambda f'(X)$$

Where $f'(X_k)$ is the gradient of $f$. However, there is a problem: Since the first order taylor expansion may not be a good approximation to $f(x)$. We could also use second order taylor expansion which is a better approximation.

### Newton Method
If we could construct the second order taylor expansion. Then we would have a better approximation of $f(X)$. To find its updating rule, we follow the similar procedure w.r.t gradient descent. First the second order taylor expansion with $a = X_k$

$$f(X) \approx f(X_k) + \frac{f'(X_k)}{1!}(X-X_k) + \frac{f''(X_k)}{2!}(X-X_k)^2$$

And this time we could find the optimum point of $f(X)$ since it is a convex quadratic function.

$$\frac{df(X)}{dX} = f'(X_k) + \frac{f''(X_k)}{2}(X - X_k) = 0$$

I would like to clarify $f''(X)$ was so called Hessian matrix $H(X)$. I simplified its notation but to avoid confusion. $$f''(X) = H(X) = [\frac{d^2f}{dx_idx_j}]_{n \times n}$$
Then to update the next step of $X$. We could find its optimum step would be

$$X_{k+1} \leftarrow X_k - \frac{f'(X)}{f''(X)}$$

### The advantages and disadvantages
There are advantages and disadvantages for both of the algorithms. For gradient descent, it is simple and easy to compute. But it may take longer to converge. For Newton Method, it converges much faster. However, the computation of the inverse Hessian matrix requires much longer time. Actually, the computation of the inverse Hessian Matrix is the main reason that current Machine Learning does not use such methods. However, the idea of second derivative is useful and some popular optimizers follows the same idea like Adam.
There are some other solutions for the Hessian matrix. Simulated Newton Method applied some tricks to simulate the Hessian Matrix, which was another improvement based on Newton Method. But I would leave another day to write more about this.
